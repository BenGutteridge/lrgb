out_dir: results
metric_best: mae
metric_agg: argmin
dataset:
  dir: /data/beng/datasets
  format: PyG-ZINC
  name: subset
  task: graph
  task_type: regression
  transductive: False
  node_encoder: True
  node_encoder_name: TypeDictNode
  node_encoder_num_types: 28
  node_encoder_bn: False
  edge_encoder: True
  edge_encoder_name: TypeDictEdge
  edge_encoder_num_types: 4
  edge_encoder_bn: False
train:
  mode: custom
  batch_size: 128
  eval_period: 1
  ckpt_period: 100
model:
  type: gnn
  loss_fun: l1
  edge_decoding: dot
  graph_pooling: add
gnn:
  layers_pre_mp: 1
  layers_mp: 2
  layers_post_mp: 1
  dim_inner: 300
  layer_type: gcnconv # generalconv
  stage_type: stack
  batchnorm: True
  act: prelu
  dropout: 0.0
  agg: mean
  normalize_adj: False
optim:
  optimizer: adam
  base_lr: 0.01
  max_epoch: 100

# out_dir: results
# tensorboard_each_run: False
# metric_best: ap
# # wandb:
# #   use: False
# #   project: peptides-func
# dataset:
  dir: /data/beng/datasets
#   format: PyG-ZINC
#   name: subset # full
#   task: graph
#   task_type: classification_multilabel
#   transductive: False
#   node_encoder: True
#   node_encoder_name: Atom
#   node_encoder_bn: False
#   edge_encoder: False
# train:
#   mode: custom
#   batch_size: 128
#   eval_period: 1
#   ckpt_period: 100
# model:
#   type: gnn
#   loss_fun: cross_entropy
#   graph_pooling: mean
# gnn:
#   layers_pre_mp: 0
#   layers_mp: 5
#   layers_post_mp: 1
#   dim_inner: 300
#   layer_type: gcnconv
#   stage_type: stack
#   batchnorm: True
#   act: relu
#   dropout: 0.0
#   agg: mean
#   normalize_adj: False
# optim:
#   optimizer: adamW
#   weight_decay: 0.0
#   base_lr: 0.001
#   max_epoch: 500
#   scheduler: reduce_on_plateau
#   reduce_factor: 0.5
#   schedule_patience: 20
#   min_lr: 1e-5
